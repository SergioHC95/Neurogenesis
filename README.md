# Grow When Needed: Adaptive Capacity Expansion in Neural Networks

## ðŸ§  Project Overview

**Goal:** Design MLPs that start small and *grow adaptively* during training, based on internal signals that reflect learning saturation and bottlenecks. We aim to boost *sample efficiency*, reduce *computational cost*, and maintain strong generalization.

This project explores how internal training signalsâ€”such as mutual information, gradient norms, and activation dynamicsâ€”can be used to **diagnose capacity limits** and **trigger model growth**.

## ðŸ”¬ Motivation

Modern networks are often overparameterized from the outset. But this is wasteful. We hypothesize that:
> A model should only grow **when it needs to**â€”not before.

This principle can:
- Save compute and memory
- Adapt to different dataset complexities
- Offer interpretability through clear growth triggers

## ðŸ§ª Key Research Questions

1. What internal signals reliably detect saturation in learning dynamics?
2. Does adaptive growth improve compute/sample efficiency?
3. What growth strategies are most effective (depth-first, width-first)?
4. How should newly added neurons/layers be initialized?

## ðŸ§­ Project Structure

```
grow-when-needed/
â”‚
â”œâ”€â”€ configs/           # YAML or JSON experiment configurations
â”œâ”€â”€ notebooks/         # Analysis & development notebooks
â”œâ”€â”€ src/               # Main codebase
â”‚   â”œâ”€â”€ models/        # MLPs (static and dynamic)
â”‚   â”œâ”€â”€ diagnostics/   # Mutual info, gradient norms, activation stats
â”‚   â”œâ”€â”€ data/          # Loaders and synthetic datasets
â”‚   â”œâ”€â”€ training/      # Trainer, growth triggers, callbacks
â”‚   â”œâ”€â”€ evaluation/    # Metrics and analysis
â”‚   â””â”€â”€ utils/         # Logging, seeding, helpers
â”‚
â”œâ”€â”€ experiments/       # Run scripts for main experiments
â”œâ”€â”€ results/           # Logs, plots, checkpoints
â”œâ”€â”€ tests/             # Unit tests
â”œâ”€â”€ paper/             # Draft paper and figures
â”‚
â”œâ”€â”€ requirements.txt   # Dependencies
â”œâ”€â”€ setup.py           # Install as package
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ðŸ“Š Planned Experiments

- MNIST, Fashion-MNIST, Parity Task, CIFAR-10 (flattened)
- Baselines: small/fixed MLPs, dropout, early stopping
- Ablations: MI vs gradient triggers, growth type, layer init

## ðŸ“¦ Deliverables

- Modular PyTorch codebase
- Diagnostic toolkit
- Paper-ready writeup
- Visualizations and benchmark results

---

Let the model grow *only when it needs to*. ðŸŒ±
